{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extremum Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Definition (Extremum Estimator)**: An estimator $\\hat{\\theta}$ is called an extremum estimator if there is a scalar objective function $Q_n(\\bf{w};\\theta)$ such that\n",
    "$$\\hat{\\theta} \\in \\arg \\max Q_n(\\bf{w};\\theta)$$\n",
    "subject to $\\theta \\in \\Theta \\subset \\mathbb{R}^p$, where\n",
    "- $n$ is the number of observations in the data\n",
    "- $\\bf{w} \\equiv (\\bf{w}_1,\\dots,\\bf{w}_n)$ is the sample or the data, and \n",
    "- $\\Theta$ is the set of possible parameter values\n",
    "\n",
    "This maximization problem may not necessarily have a solution. The following lemma shows that $\\hat{\\theta}$ is measurable if $Q_n(\\theta)$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lemma (Existence of Extremum Estimators)**: Suppose that\n",
    "1. the parameter space $\\Theta$ is a compact subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\theta)$ is continuous in $\\theta$ for any data $\\bf{w}$, and\n",
    "3. $Q_n(\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$.\n",
    "\n",
    "Then there exists $\\hat{\\theta}$ such that $\\arg \\max Q_n(\\bf{w};\\theta)$ subject to  $\\theta \\in \\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Classes of Extremum Estimators\n",
    "1. M-Estimators:  $Q_n(\\theta)$ is a simple averate\n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)$$\n",
    "    - Examples: maximum likelihood (ML) and nonlinear least squares (NLS)\n",
    "2. Generalized Method of Moments (GMM)\n",
    "$$Q_n(\\theta)=-g_n(\\theta)'\\hat{\\bf{W}}g_n(\\theta)$$\n",
    "where\n",
    "    - $\\hat{\\bf{W}}$ is a $K \\times K$ symmetric and positive definite matrix that defines the distance of $g_n(\\theta)$ from zero.\n",
    "    - $g_n(\\theta) = \\frac{1}{n}\\sum_{1}^{n}g(\\bf{w}_i;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Maximum Likelihood\n",
    "- ${\\bf{w}_i}$ is i.i.d.\n",
    "- $\\theta$ is a finite-dimensional vector\n",
    "- a functional form of $f({\\bf{w}_i};\\theta)$ is known\n",
    "- $\\theta_0$ is the true parameter value\n",
    "\n",
    "The joint density of data $(\\bf{w}_1,\\dots,\\bf{w}_n)$ is \n",
    "$$f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta_0)=\\prod_{1}^{n} f(\\bf{w}_i;\\theta_0)$$\n",
    "The $Q_n(\\theta)$ can either be the likelihood and the log-likelihood function:\n",
    "$$f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta)=\\prod_{1}^{n} f(\\bf{w}_i;\\theta)$$\n",
    "$$\\log f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta)=\\log \\left[ \\prod_{1}^{n} f(\\bf{w}_i;\\theta) \\right] = \\sum_{1}^{n} \\log f(\\bf{w}_i;\\theta) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Conditional Maximum Likelihood\n",
    "- ${\\bf{w}_i}$ is partitioned into two groups, $y_i$ an $\\bf{x}_i$, and the interest is to examine how $\\bf{x}_i$ influences the conditional distribution of $y_i$\n",
    "- $f(y_i |\\bf{x}_i; \\psi_0)$ be the conditional density of $y_i$ given $\\bf{x}_i$ \n",
    "- $f(\\bf{x}_i; \\psi_0)$ be the marginal density of $\\bf{x}_i$\n",
    "\n",
    "The joint density of data $(\\bf{w}_1,\\dots,\\bf{w}_n) = (y_t,\\bf{x}'_i)' $ is \n",
    "$$ f(y_t ,\\bf{x}_i;\\theta_0,\\psi_0) = f(y_i | \\bf{x}_i;\\theta_0)f(\\bf{x}_i;\\psi_0) $$\n",
    "The $Q_n(\\theta)$ can either be the likelihood and the log-likelihood function:\n",
    "$$f(\\bf{w}_i;\\theta,\\psi)=\\prod_{1}^{n} f(y_i|\\bf{x}_i;\\theta) + \\prod_{1}^{n} f(\\bf{x}_i;\\psi)$$\n",
    "$$\\sum_{1}^{n} \\log f(\\bf{w}_i;\\theta,\\psi)=\\sum_{1}^{n} \\log f(y_i|\\bf{x}_i;\\theta) + \\sum_{1}^{n} \\log f(\\bf{x}_i;\\psi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Nonlinear least square\n",
    "- $y_i = \\varphi_i(\\bf{x}_i; \\psi_0) + \\epsilon_i$\n",
    "- $\\mathbb{E}(\\epsilon_i|\\bf{x}_i)$\n",
    "- The functional form of $\\varphi$ is known\n",
    "\n",
    "The $Q_n(\\theta)$ is\n",
    "$$-\\frac{1}{n}\\sum_{1}^{n}\\left[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\right]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Nonlinear GMM\n",
    "- $y_i = \\varphi_i(\\bf{x}_i; \\psi_0) + \\epsilon_i$\n",
    "- $\\mathbb{E}(\\epsilon_i|\\bf{x}_i)$\n",
    "- The functional form of $\\varphi$ is known\n",
    "\n",
    "Moment condition:\n",
    "$$\\mathbb{E}(\\epsilon_i | \\bf{x}_i)=0 \\rightarrow \\mathbb{E}(\\epsilon_i \\cdot \\bf{x}_i)=0 \\rightarrow \\mathbb{E}\\bigg( \\big[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\big] \\cdot \\bf{x}_i \\bigg)=0$$\n",
    "Using the moment condition, the $Q_n(\\theta)$ is\n",
    "$$Q_n(\\theta)=-g_n(\\theta)'\\hat{\\bf{W}}g_n(\\theta)$$\n",
    "where\n",
    "$$g_n(\\theta) = \\frac{1}{n}\\sum_{1}^{n}\\big[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\big] \\cdot \\bf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consistency\n",
    "If the parameter space is compact,\n",
    "\n",
    "**Proposition (Consistency with Compact Parameter Space)**: Suppose that \n",
    "1. $\\Theta$ is a compact subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\bf{w};\\theta)$ is a continuous function of $\\theta$ for any data $\\bf{w}$\n",
    "3. $Q_n(\\bf{w};\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "4. If there is a function $Q_0(\\theta)$ such that\n",
    "    - (identification)  $Q_0(\\theta)$ is uniquely maximized at $\\theta_0 \\in \\Theta$\n",
    "    - (uniform convergence) $\\sup_{\\theta \\in \\Theta} \\vert Q_n(\\theta) - Q_0(\\theta) \\vert \\rightarrow_{p} 0 $\n",
    "    \n",
    "Then, $\\hat{\\theta} \\rightarrow_{p} \\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the parameter space is not compact,\n",
    "\n",
    "**Proposition (Consistency without Compact Parameter Space)**: Suppose that \n",
    "1. $\\theta_0 \\in \\text{interior} \\Theta$ and $\\Theta$ is a convex subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\bf{w};\\theta)$ is a concave over $\\Theta$ of for any data $\\bf{w}$\n",
    "3. $Q_n(\\bf{w};\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "4. If there is a function $Q_0(\\theta)$ such that\n",
    "    - (identification)  $Q_0(\\theta)$ is uniquely maximized at $\\theta_0 \\in \\Theta$\n",
    "    - (point-wise convergence) $\\vert Q_n(\\theta) - Q_0(\\theta) \\vert \\rightarrow_{p} 0$ for all $\\theta \\in \\Theta$\n",
    "    \n",
    "Then, $\\hat{\\theta} \\rightarrow_{p} \\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above proposition presents the set of sufficient conditions under which an extremum estimator is consistent. Now, let's specialize these conditions to M-estimators and GMM estimators.\n",
    "\n",
    "1. What is $Q_n(\\theta)$ for M-Estimators and GMM?\n",
    "2. What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "3. What is the identification condition for an M-estimator?\n",
    "4. What is the uniform/point-wise convergence condition and the point-wise convergence condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q1) What is $Q_0(\\theta)$ in the previous consistency propositions?\n",
    "For M-estimator, the objective function is:\n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)$$\n",
    "\n",
    "If $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$ exists and is finite, \n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)\\rightarrow_{p} \\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$$\n",
    "\n",
    "Therefore, \n",
    "$$Q_0(\\theta)=\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q2) What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "- If $\\Theta$ is compact,\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a continuous function of $\\theta$ for any data $\\bf{w}$\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "- If $\\Theta$ is not compact, but is convex and $\\theta \\in \\text{interior} \\Theta$:\n",
    "    - $m(\\bf{w}_i;\\theta)$ is concave over $\\Theta$ for any data $\\bf{w}$\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q3)  What is the identification condition for an M-estimator?\n",
    "\n",
    "Identification condition for M-estimator is $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$ is uniquelyidentified at $\\theta_0 \\in \\Theta$\n",
    "- For ML, where $m(\\bf{w}_i;\\theta)=\\log f(y_i \\vert \\bf{x}_i;\\theta_0)$, for all $\\theta \\neq \\theta_0$,\n",
    "$$\\log f(y_i \\vert \\bf{x}_i;\\theta) \\neq \\log f(y_i \\vert \\bf{x}_i;\\theta_0) $$\n",
    "\n",
    "- For NLS, where $m(\\bf{w}_i;\\theta)=-\\left[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\right]^2$, for all $\\theta \\neq \\theta_0$,\n",
    "$$\\varphi(\\bf{x_i};\\theta) \\neq \\varphi(\\bf{x_i};\\theta_0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q4) What is the uniform and point-wise convergence conditions?\n",
    "- Uniform convergence condition: by the Law of the Large Numbers, the condition becomes\n",
    "$$\\mathbb{E}\\left[\\sup_{\\theta \\in \\Theta} \\vert m(\\bf{w}_i;\\theta) \\vert \\right] < \\infty$$\n",
    "- Point-wise convergence condition: by the Ergodic Theorem, the condition becomes\n",
    "$$\\mathbb{E}\\left[\\vert m(\\bf{w}_i;\\theta) \\vert \\right] < \\infty$$\n",
    "for all $\\theta \\in \\Theta$, (i.e., $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta) \\right]$ exists and is finite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of GMM Estimator \n",
    "#### (Q1) What is $Q_0(\\theta)$ in the previous consistency propositions?\n",
    "\n",
    "For GMM estimator, the objective function is:\n",
    "$$Q_n(\\theta)=-\\bigg[\\frac{1}{n}\\sum_{1}^{n} g_n(\\bf{w}_i;\\theta)\\bigg]'\\hat{\\bf{W}}\\bigg[\\frac{1}{n}\\sum_{1}^{n} g_n(\\bf{w}_i;\\theta)\\bigg]$$\n",
    "$$Q_0(\\theta)=-\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]'\\hat{\\bf{W}}\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]$$\n",
    "\n",
    "#### (Q2) What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "1. $g(\\bf{w}_i;\\theta)$ is a continuous function of $\\theta$ for any data $\\bf{w}$\n",
    "2. $g(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of GMM Estimator \n",
    "#### (Q3)  What is the identification condition for an GMM estimator?\n",
    "\n",
    "- Notice that the maximum is zero at $\\theta_0$, because of the orthogonality conditions, $\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]=0$.\n",
    "- Therefore, the identification is satisfied if for all $\\theta \\in \\Theta$, \n",
    "$$\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big] \\neq \\mathbb{E}\\big[g(\\bf{w}_i;\\theta_0)\\big]$$\n",
    "\n",
    "#### (Q4) What is the uniform convergence condition?\n",
    "$$\\mathbb{E}\\left[\\sup_{\\theta \\in \\Theta} \\vert\\vert g(\\bf{w}_i;\\theta) \\vert\\vert \\right] < \\infty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aymptotic Normality\n",
    "\n",
    "## The General Framework\n",
    "\n",
    "- $\\hat{\\theta} = \\arg \\max Q_n(\\theta)$\n",
    "- If $\\bar{\\theta} \\in [\\theta_0,\\hat{\\theta}]$, [Mean Value Theorem](https://en.wikipedia.org/wiki/Mean_value_theorem) or first order Taylor Expansion:\n",
    "$$0 = \\frac{\\partial{Q_n(\\hat{\\theta})}}{\\partial{\\theta}}  = \\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}} + \n",
    "    \\frac{\\partial{Q^2_n(\\bar{\\theta})}}{\\partial{\\theta}\\partial{\\theta'}}(\\hat{\\theta}-\\theta_0) $$\n",
    "- If $\\frac{\\partial{Q^2_n(\\bar{\\theta})}}{\\partial{\\theta}\\partial{\\theta'}}$ is [nonsingular](https://mathworld.wolfram.com/NonsingularMatrix.html#:~:text=A%20square%20matrix%20that%20is,45) and $\\frac{\\partial{Q_n(\\hat{\\theta})}}{\\partial{\\theta}}=0$, then\n",
    "$$\\sqrt{n}(\\hat{\\theta}-\\theta_0) \n",
    "= -\\bigg[\\frac{\\partial{Q^2_n(\\bar{\\theta})}}\n",
    "{\\partial{\\theta}\\partial{\\theta'}}\\bigg]^{-1}\n",
    "\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\begin{align}\n",
    "\\sqrt{n}(\\hat{\\theta}-\\theta_0) \n",
    "&= -\\bigg[\\frac{\\partial{Q^2_n(\\theta_0)}}\n",
    "{\\partial{\\theta}\\partial{\\theta'}}\\bigg]^{-1}\n",
    "\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}} \\\\\n",
    "&\\xrightarrow{d} N(0,A^{-1}BA^{-1})\n",
    "\\end{align}$$\n",
    "where\n",
    "$$A = \\frac{\\partial{Q^2_n(\\theta_0)}}{\\partial{\\theta}\\partial{\\theta'}}$$\n",
    "$$B = \\mathrm{Var}\\left(\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asymptotic Normality for M-Estimators\n",
    "\n",
    "\n",
    "Let's denote\n",
    "- **Score vector** as $$\\bf{s}(\\bf{w_i};\\theta) = \\frac{\\partial{Q_n(\\theta)}}{\\partial{\\theta}} = \\frac{\\partial{m(\\bf{w_i};\\theta)}}{\\partial{\\theta}}$$ \n",
    "- **Hessian** as $$\\bf{H}(\\bf{w_i};\\theta) = \\frac{\\partial{Q^2_n(\\theta)}}\n",
    "{\\partial{\\theta}\\partial{\\theta'}} = \\frac{\\partial^2{m(\\bf{w_i};\\theta)}}{\\partial{\\theta}\\partial{\\theta'}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$\\frac{1}{n}\\sum_{1}^{n}\\bf{H}(\\bf{w_i};\\bar{\\theta}) \\xrightarrow{p} \\mathbb{E}\\left[\\bf{H}(\\bf{w_i};\\theta_0)\\right]$$\n",
    "$$\\frac{1}{\\sqrt{n}}\\sum_{1}^{n}\\bf{s}(\\bf{w_i};\\theta_0)\\xrightarrow{d} N(0,\\Sigma)$$\n",
    "Then by [Slutzky's theorem](https://en.wikipedia.org/wiki/Slutsky%27s_theorem),\n",
    "$$\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\rightarrow_{d} N\\Bigg( 0,\\mathbb{E}\\big[\\bf{H}(\\bf{w_i};\\theta_0)\\big]^{-1}\\; \\Sigma \\; \\mathbb{E}\\big[\\bf{H}(\\bf{w_i};\\theta_0)\\big]^{-1} \\Bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Asymptotic Normality for GMM-Estimators\n",
    "$$Q_n(\\theta) = g_n(\\theta)'Wg_n(\\theta)$$ where\n",
    "$$g_n(\\theta) = \\frac{1}{n}\\sum_{1}^{n}g(w_i;\\theta)$$\n",
    "Let $G_n(\\theta)$ is the Jacobian of $g_n(\\theta)$\n",
    "$$\\bf{G}_n(\\theta) = \\frac{\\partial g_n(\\theta)}{\\partial \\theta } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If $\\bar{\\theta} \\in [\\theta_0,\\hat{\\theta}]$,\n",
    "$$\\begin{align}0 = \\bf{G}_n(\\hat{\\theta})'\\bf{W}g_n(\\hat{\\theta}) & = \\bf{G}_n(\\hat{\\theta})'\\bf{W}\\bigg(g_n(\\theta_0) + \\bf{G}_n(\\bar{\\theta})\\big(\\hat{\\theta}-\\theta_0\\big)\\bigg) \\\\\n",
    "& = \\bf{G}_n(\\hat{\\theta})'\\bf{W}g_n(\\theta_0) + \\bf{G}_n(\\hat{\\theta})'\\bf{W}\\bf{G}_n(\\bar{\\theta})\\big(\\hat{\\theta}-\\theta_0\\big)\n",
    "\\end{align}$$\n",
    "because $Q_n(\\theta)$ is already a quadratic form in $g_n(\\theta)$\n",
    "- If $\\bf{G}_n(\\hat{\\theta})'\\bf{W}\\bf{G}_n(\\bar{\\theta})$ is nonsingular, then\n",
    "$$\\sqrt{n}(\\hat{\\theta}-\\theta_0) \n",
    "= -\\big[\\bf{G}_n(\\hat{\\theta})'\\bf{W}\\bf{G}_n(\\bar{\\theta})\\big]^{-1}\n",
    "\\bf{G}_n(\\hat{\\theta})'\\bf{W}\\sqrt{n}g_n(\\theta_0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $G=\\mathbb{E}\\big[G_n(\\theta_0)\\big]$ and \n",
    "$\\Omega=\\mathbb{E}=\\big[g(\\bf{w};\\theta_0)g(\\bf{w};\\theta_0)'\\big]$\n",
    "$$\\begin{align}\n",
    "\\sqrt{n}(\\hat{\\theta}-\\theta_0) & = (G'WG)^{-1}G'W\\sqrt{n}g_n(\\theta_0) \\\\\n",
    "& = (G'WG)^{-1}G'WN(0,\\Omega) \\\\\n",
    "& = N\\bigg(0,(G'WG)^{-1}G'W \\Omega WG (G'WG)^{-1}\\bigg) \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**What is the optimal choice of the weighting matrix $W$?**\n",
    "- The most efficient choice of $W = \\Omega^{-1}$\n",
    "$$\\begin{align}\n",
    "\\sqrt{n}(\\hat{\\theta}-\\theta_0) & = \n",
    "N\\bigg(0,(G'\\Omega^{-1}G)^{-1}G'\\Omega^{-1} \\Omega \\Omega^{-1} G (G'\\Omega^{-1} G)^{-1}\\bigg) \\\\\n",
    "& \\xrightarrow{d} N\\bigg(0,(G'\\Omega^{-1}G)^{-1}\\bigg)\n",
    "\\end{align}$$\n",
    "- When $G$ is invertible, $W$ is irrelevant\n",
    "$$\\begin{align}\n",
    "\\sqrt{n}(\\hat{\\theta}-\\theta_0) & = N\\bigg(0,G^{-1}\\Omega G'^{-1}\\bigg) \\\\\n",
    "& \\xrightarrow{d} N\\bigg(0,(G'\\Omega^{-1}G)^{-1}\\bigg)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM vs. ML\n",
    "$$ \\mathrm{Avar}(\\hat{\\theta})\\geq\\mathbb{E}\\big[\\bf{s}(\\bf{w_i};\\theta_0)\\bf{s}(\\bf{w_i};\\theta_0)'\\big]^{-1}$$\n",
    "where \n",
    "$$\\bf{s}(\\bf{w_i};\\theta_0) \\equiv \\frac{\\partial \\log f(\\bf{w}_i;\\theta_0)}{\\partial \\theta}$$\n",
    "- The lower bound for the asymptotic variance of GMM estimators is asymptotic variance of the ML estimator.\n",
    "- ML is more efficient than GMM in general\n",
    "- GMM with the optimal orthogonal condition is numerically equivalent to ML\n",
    "- ML exploits the knowledge of the parametric form of $f(\\bf{w}_i;\\theta)$ while GMM doesn't \n",
    "- GMM is more robust than ML to the specification error in $f(\\bf{w}_i;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Restrictions and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Restrictions\n",
    "\n",
    "Let $\\hat{\\theta}$ be the extremum estimator in either ML or GMM. The constrained estimator, denoted $\\tilde{\\theta}$, solves\n",
    "$$\\max_{\\theta \\in \\Theta} Q_n(\\theta) \\quad s.t. \\quad \\bf{a}(\\theta)=\\bf{0}$$\n",
    "\n",
    "In many cases, economic theory suggests restrictions on the parameters\n",
    "of a model. For example, a demand function is supposed to be homogeneous\n",
    "of degree zero in prices and income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The general formulation of linear equality restrictions is the model\n",
    "$$\\begin{align}\n",
    "y&=X\\beta+\\epsilon \\\\\n",
    "R\\beta&=r\n",
    "\\end{align}$$\n",
    "- We assume $R$ is of rankn $Q$, so that there are no redundant restrictions\n",
    "- We also assume that $\\exists \\; \\beta$ that satisfies the restrictions: they aren't infeasible\n",
    "Taking Lagrangean,\n",
    "$$\\min_{\\beta,\\lambda} Q_n(\\beta,\\lambda)\n",
    "= \\frac{1}{n}\\big(y-X\\beta\\big)' + 2\\lambda'\\big(R\\beta-r\\big)$$\n",
    "\n",
    "$$H_0: R\\beta_0=r$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "In many cases, one wishes to test economic theories. If theory suggests parameter restrictions,\n",
    "as in the above homogeneity example, one can test theory by testing parameter restrictions. A number of tests are available.\n",
    "- Wald\n",
    "- Lagrange multiplier (LM) - for constrained estimator\n",
    "- Likelihood ratio (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "There is a trio of statistics called **the trinity**:\n",
    "1. Wald - for unconstrained estimator\n",
    "2. Lagrange multiplier (LM) - for constrained estimator\n",
    "3. Likelihood ratio (LR)\n",
    "\n",
    "that can be used for testing the null hypothesis.\n",
    "\n",
    "- The three statistics share the same asymptotic distribution (of $\\chi^2$)\n",
    "- Applicable for both ML and GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Null Hypothesis\n",
    "Consider the problem of testing a set of $r$ possibly nonlinear\n",
    "restrictions and $p$-dimensional model parameter:\n",
    "$$ H_0 : \\bf{a}(\\theta_0) = 0$$\n",
    "\n",
    "- $\\bf{a}(\\theta_0)$ has dimension $(r \\times 1)$\n",
    "- $\\bf{A}(\\theta)$ has dimension $(r \\times p)$\n",
    "\n",
    "Assume\n",
    "- $\\bf{a}(\\cdot)$ is continuously differentiable \n",
    "- $\\bf{A}(\\theta)$ is the Jacobian of $\\bf{a}(\\theta)$\n",
    "$$\\bf{A}(\\theta) = \\frac{\\partial \\bf{a}(\\theta)}{\\partial \\theta'}$$\n",
    "- $\\bf{A}(\\theta)$ is of full (row) rank (i.e. r restrictions are not\n",
    "redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assumptions for the Trinity\n",
    "1. MVT or Taylor expansion for the sampling error:\n",
    "$$\\sqrt{n}\\big(\\hat{\\theta}-\\theta_0\\big) = \\Psi^{-1}\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}+o_p$$\n",
    "where the term $o_p$ means some random variable that converges to zero in probability, which will depend on the context. \n",
    "2. $\\quad$\n",
    "$$\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}\n",
    "    \\xrightarrow{d} N\\big(0,\\Sigma\\big)$$\n",
    "3. $\\sqrt{n}\\big(\\tilde{\\theta}-\\theta_0\\big)$ converges in distribution to a random variable, where $\\tilde{\\theta}$ is the constrained estimator:\n",
    "$$\\tilde{\\theta} \\in \\arg \\max_{\\theta \\in \\Theta} Q_n(\\theta) \\quad s.t. \\quad \\bf{a}(\\theta)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sqrt{n}\\big(\\hat{\\theta}-\\theta_0\\big) = \\Psi^{-1}\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}+o_p$$\n",
    "Recall\n",
    "- For M-estimator:\n",
    "$$\\sqrt{n}(\\hat{\\theta}-\\theta_0) \n",
    "= -\\bigg[\\frac{\\partial{Q^2_n(\\bar{\\theta})}}\n",
    "{\\partial{\\theta}\\partial{\\theta'}}\\bigg]^{-1}\n",
    "\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}$$\n",
    "$$\\Psi = \\frac{\\partial{Q^2_n(\\bar{\\theta})}}\n",
    "{\\partial{\\theta}\\partial{\\theta'}} = \\mathbb{E}\\big[\\bf{H}(\\bf{w_i};\\bar{\\theta})\\big]$$\n",
    "\n",
    "- For GMM: \n",
    "$$\\sqrt{n}(\\hat{\\theta}-\\theta_0) \n",
    "= -\\big[\\bf{G}_n(\\hat{\\theta})'\\hat{\\bf{W}}\\bf{G}_n(\\bar{\\theta})\\big]^{-1}\n",
    "\\bf{G}_n(\\hat{\\theta})'\\hat{\\bf{W}}\\sqrt{n}g_n(\\theta_0)$$\n",
    "$$\\Psi = \\bf{G}_n(\\hat{\\theta})'\\hat{\\bf{W}}\\bf{G}_n(\\bar{\\theta})$$\n",
    "\n",
    "Notice that for ML and efficient GMM ($\\bf{W}=\\bf{\\Omega^{-1}}$), then\n",
    "$$\\Sigma = -\\Psi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wald Statistic\n",
    "Based on the Mean Value Theorem and Taylor expansion, under the null:\n",
    "$$\\begin{align}\n",
    "\\sqrt{n} \\; \\bf{a}(\\hat{\\theta}) \n",
    "& = \\bf{A}(\\theta_0)\\sqrt{n} \\; (\\hat{\\theta}-\\theta_0)+o_p \\\\\n",
    "& =-\\bf{A}(\\theta_0)\\Psi^{-1}\\sqrt{n} \\; \\Psi^{-1}\\sqrt{n}\\frac{\\partial{Q_n(\\theta_0)}}{\\partial{\\theta}}+o_p+o_p\n",
    "\\end{align}$$\n",
    "and the asymptotic variance is:\n",
    "$$\\begin{align}\n",
    "\\mathrm{AVar}\\big(\\bf{a}(\\hat{\\theta})\\big)\n",
    "& = \\bf{A}(\\theta_0)\\Psi^{-1}\\Sigma\\Psi^{-1}\\bf{A}(\\theta_0) \\\\\n",
    "& = \\bf{A}(\\theta_0) \\Sigma^{-1} \\bf{A'}(\\theta_0)\n",
    "\\end{align}$$\n",
    "\n",
    "Since the $\\bf{A}_0$ and $\\Sigma$ is positive definite $\\mathrm{AVar}\\big(\\bf{a}(\\hat{\\theta})\\big)$ is positive definite. Therefore, the associated quadratic form \n",
    "$$W\\equiv n\\bf{a}(\\hat{\\theta})'\n",
    "\\big[\\bf{A}(\\hat{\\theta})\\hat{\\Sigma}^{-1}\\bf{A}(\\hat{\\theta})'\\big]^{-1}\\bf{a}(\\hat{\\theta})$$\n",
    "is asymptotically $\\chi^2(r)$ under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lagrange Multiplier (LM) Statistic\n",
    "$$\\begin{align}\n",
    "LM &\\equiv n\\bigg(\\frac{\\partial Q_n(\\tilde{\\theta})}{\\partial \\theta}\\bigg)' \\tilde{\\Sigma}^{-1} \\bigg(\\frac{\\partial Q_n(\\tilde{\\theta})}{\\partial \\theta}\\bigg) \\\\\n",
    "& = n\\gamma'_n \\big[\\bf{A}(\\hat{\\theta})\\hat{\\Sigma}^{-1}\\bf{A}(\\hat{\\theta})'\\big] \\gamma_n\n",
    "\\end{align}$$\n",
    "is asymptotically $\\chi^2(r)$ under the nuull hypothesis.\n",
    "### Likelihood Ratio Multiplier (LR) Statistic\n",
    "\n",
    "$$\\begin{align}\n",
    "LR &\\equiv 2n\\big[ Q_n(\\hat{\\theta}) - Q_n(\\tilde{\\theta}) \\big] \\\\\n",
    "& = n\\gamma'_n \\big[\\bf{A}(\\hat{\\theta})\\hat{\\Sigma}^{-1}\\bf{A}(\\hat{\\theta})'\\big] \\gamma_n\n",
    "\\end{align}$$\n",
    "is asymptotically $\\chi^2(r)$ under the null hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
