{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extreme Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "**Definition (Extreme Estimator)**: An estimator $\\hat{\\theta}$ is called an extreme estimator if there is a scalar objective function $Q_n(\\bf{w};\\theta)$ such that\n",
    "$$\\hat{\\theta} \\in \\arg \\max Q_n(\\bf{w};\\theta)$$\n",
    "subject to $\\theta \\in \\Theta \\subset \\mathbb{R}^p$, where\n",
    "- $n$ is the number of observations in the data\n",
    "- $\\bf{w} \\equiv (\\bf{w}_1,\\dots,\\bf{w}_n)$ is the sample or the data, and \n",
    "- $\\Theta$ is the set of possible parameter values\n",
    "\n",
    "This maximization problem may not necessarily have a solution. The following lemma shows that $\\hat{\\theta}$ is measurable if $Q_n(\\theta)$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Lemma (Existence of Extreme Estimators)**: Suppose that\n",
    "1. the parameter space $\\Theta$ is a compact subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\theta)$ is continuous in $\\theta$ for any data $\\bf{w}$, and\n",
    "3. $Q_n(\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$.\n",
    "\n",
    "Then there exists $\\hat{\\theta}$ such that $\\arg \\max Q_n(\\bf{w};\\theta)$ subject to  $\\theta \\in \\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Classes of Extreme Estimators\n",
    "1. M-Estimators:  $Q_n(\\theta)$ is a simple averate\n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)$$\n",
    "    - Examples: maximum likelihood (ML) and nonlinear least squares (NLS)\n",
    "2. Generalized Method of Moments (GMM)\n",
    "$$Q_n(\\theta)=-g_n(\\theta)'\\hat{\\bf{W}}g_n(\\theta)$$\n",
    "where\n",
    "    - $\\hat{\\bf{W}}$ is a $K \\times K$ symmetric and positive definite matrix that defines the distance of $g_n(\\theta)$ from zero.\n",
    "    - $g_n(\\theta) = \\frac{1}{n}\\sum_{1}^{n}g(\\hat{\\bf{W}};\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Maximum Likelihood\n",
    "- ${\\bf{w}_i}$ is i.i.d.\n",
    "- $\\theta$ is a finite-dimensional vector\n",
    "- a functional form of $f({\\bf{w}_i};\\theta)$ is known\n",
    "- $\\theta_0$ is the true parameter value\n",
    "\n",
    "The joint density of data $(\\bf{w}_1,\\dots,\\bf{w}_n)$ is \n",
    "$$f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta_0)=\\prod_{1}^{n} f(\\bf{w}_i;\\theta_0)$$\n",
    "The $Q_n(\\theta)$ can either be the likelihood and the log-likelihood function:\n",
    "$$f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta)=\\prod_{1}^{n} f(\\bf{w}_i;\\theta)$$\n",
    "$$\\log f(\\bf{w}_1,\\dots,\\bf{w}_n;\\theta)=\\log \\left[ \\prod_{1}^{n} f(\\bf{w}_i;\\theta) \\right] = \\sum_{1}^{n} \\log f(\\bf{w}_i;\\theta) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Conditional Maximum Likelihood\n",
    "- ${\\bf{w}_i}$ is partitioned into two groups, $y_i$ an $\\bf{x}_i$, and the interest is to examine how $\\bf{x}_i$ influences the conditional distribution of $y_i$\n",
    "- $f(y_i |\\bf{x}_i; \\psi_0)$ be the conditional density of $y_i$ given $\\bf{x}_i$ \n",
    "- $f(\\bf{x}_i; \\psi_0)$ be the marginal density of $\\bf{x}_i$\n",
    "\n",
    "The joint density of data $(\\bf{w}_1,\\dots,\\bf{w}_n) = (y_t,\\bf{x}'_i)' $ is \n",
    "$$ f(y_t ,\\bf{x}_i;\\theta_0,\\psi_0) = f(y_i | \\bf{x}_i;\\theta_0)f(\\bf{x}_i;\\psi_0) $$\n",
    "The $Q_n(\\theta)$ can either be the likelihood and the log-likelihood function:\n",
    "$$f(\\bf{w}_i;\\theta,\\psi)=\\prod_{1}^{n} f(y_i|\\bf{x}_i;\\theta) + \\prod_{1}^{n} f(\\bf{x}_i;\\psi)$$\n",
    "$$\\sum_{1}^{n} \\log f(\\bf{w}_i;\\theta,\\psi)=\\sum_{1}^{n} \\log f(y_i|\\bf{x}_i;\\theta) + \\sum_{1}^{n} \\log f(\\bf{x}_i;\\psi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Nonlinear least square\n",
    "- $y_i = \\varphi_i(\\bf{x}_i; \\psi_0) + \\epsilon_i$\n",
    "- $\\mathbb{E}(\\epsilon_i|\\bf{x}_i)$\n",
    "- The functional form of $\\varphi$ is known\n",
    "\n",
    "The $Q_n(\\theta)$ is\n",
    "$$-\\frac{1}{n}\\sum_{1}^{n}\\left[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\right]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### M-Estimator Example: Nonlinear GMM\n",
    "- $y_i = \\varphi_i(\\bf{x}_i; \\psi_0) + \\epsilon_i$\n",
    "- $\\mathbb{E}(\\epsilon_i|\\bf{x}_i)$\n",
    "- The functional form of $\\varphi$ is known\n",
    "\n",
    "Moment condition:\n",
    "$$\\mathbb{E}(\\epsilon_i | \\bf{x}_i)=0 \\rightarrow \\mathbb{E}(\\epsilon_i \\cdot \\bf{x}_i)=0 \\rightarrow \\mathbb{E}\\bigg( \\big[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\big] \\cdot \\bf{x}_i \\bigg)=0$$\n",
    "Using the moment condition, the $Q_n(\\theta)$ is\n",
    "$$Q_n(\\theta)=-g_n(\\theta)'\\hat{\\bf{W}}g_n(\\theta)$$\n",
    "where\n",
    "$$g_n(\\theta) = \\frac{1}{n}\\sum_{1}^{n}\\big[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\big] \\cdot \\bf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Consistency\n",
    "If the parameter space is compact,\n",
    "\n",
    "**Proposition (Consistency with Compact Parameter Space)**: Suppose that \n",
    "1. $\\Theta$ is a compact subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\bf{w};\\theta)$ is a continuous function of for any data $\\bf{w}$\n",
    "3. $Q_n(\\bf{w};\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "4. If there is a function $Q_0(\\theta)$ such that\n",
    "    - (identification)  $Q_0(\\theta)$ is uniquely maximized at $\\theta_0 \\in \\Theta$\n",
    "    - (uniform convergence) $\\sup_{\\theta \\in \\Theta} \\vert Q_n(\\theta) - Q_0(\\theta) \\vert \\rightarrow_{p} 0 $\n",
    "    \n",
    "Then, $\\hat{\\theta} \\rightarrow_{p} \\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the parameter space is not compact,\n",
    "\n",
    "**Proposition (Consistency without Compact Parameter Space)**: Suppose that \n",
    "1. $\\theta_0 \\in \\text{interior} \\Theta$ and $\\Theta$ is a convex subset of $\\mathbb{R}^p$\n",
    "2. $Q_n(\\bf{w};\\theta)$ is a concave over $\\Theta$ of for any data $\\bf{w}$\n",
    "3. $Q_n(\\bf{w};\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "4. If there is a function $Q_0(\\theta)$ such that\n",
    "    - (identification)  $Q_0(\\theta)$ is uniquely maximized at $\\theta_0 \\in \\Theta$\n",
    "    - (point-wise convergence) $\\vert Q_n(\\theta) - Q_0(\\theta) \\vert \\rightarrow_{p} 0$ for all $\\theta \\in \\Theta$\n",
    "    \n",
    "Then, $\\hat{\\theta} \\rightarrow_{p} \\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. What is Q_n(\\theta) for M-Estimators and GMM?\n",
    "2. What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "3. What is the identification condition for an M-estimator?\n",
    "4. What is the uniform/point-wise convergence condition and the point-wise convergence condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q1) What is $Q_0(\\theta)$ in the previous consistency propositions?\n",
    "For M-estimator, the objective function is:\n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)$$\n",
    "\n",
    "If $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$ exists and is finite, \n",
    "\n",
    "by Ergodic Theorem, \n",
    "$$Q_n(\\theta)=\\frac{1}{n}\\sum_{1}^{n}m(\\bf{w}_i;\\theta)\\rightarrow_{p} \\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$$\n",
    "\n",
    "Therefore, \n",
    "$$Q_0(\\theta)=\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q2) What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "- If $\\Theta$ is compact,\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a continuous function of $\\theta$ for any data $\\bf{w}$\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n",
    "- If $\\Theta$ is not compact, but is convex and $\\theta \\in \\text{interior} \\Theta$:\n",
    "    - $m(\\bf{w}_i;\\theta)$ is concave over $\\Theta$ for any data $\\bf{w}$\n",
    "    - $m(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q3)  What is the identification condition for an M-estimator?\n",
    "\n",
    "Identification condition for M-estimator is $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta)\\right]$ is uniquelyidentified at $\\theta_0 \\in \\Theta$\n",
    "- For ML, where $m(\\bf{w}_i;\\theta)=\\log f(y_i \\vert \\bf{x}_i;\\theta_0)$, for all $\\theta \\neq \\theta_0$,\n",
    "$$\\log f(y_i \\vert \\bf{x}_i;\\theta) \\neq \\log f(y_i \\vert \\bf{x}_i;\\theta_0) $$\n",
    "\n",
    "- For NLS, where $m(\\bf{w}_i;\\theta)=-\\left[ y_i - \\varphi_i(\\bf{x}_i; \\psi) \\right]^2$, for all $\\theta \\neq \\theta_0$,\n",
    "$$\\varphi(\\bf{x_i};\\theta) \\neq \\varphi(\\bf{x_i};\\theta_0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of M-Estimators\n",
    "#### (Q4) What is the uniform and point-wise convergence conditions?\n",
    "- Uniform convergence condition: by the Law of the Large Numbers, the condition becomes\n",
    "$$\\mathbb{E}\\left[\\sup_{\\theta \\in \\Theta} \\vert m(\\bf{w}_i;\\theta) \\vert \\right] < \\infty$$\n",
    "- Point-wise convergence condition: by the Ergodic Theorem, the condition becomes\n",
    "$$\\mathbb{E}\\left[\\vert m(\\bf{w}_i;\\theta) \\vert \\right] < \\infty$$\n",
    "for all $\\theta \\in \\Theta$, (i.e., $\\mathbb{E}\\left[m(\\bf{w}_i;\\theta) \\right]$ exists and is finite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of GMM Estimator \n",
    "#### (Q1) What is $Q_0(\\theta)$ in the previous consistency propositions?\n",
    "\n",
    "For GMM estimator, the objective function is:\n",
    "$$Q_n(\\theta)=-\\bigg[\\frac{1}{n}g_n(\\bf{w}_i;\\theta)\\bigg]'\\hat{\\bf{W}}\\bigg[\\frac{1}{n} g_n(\\bf{w}_i;\\theta)\\bigg]$$\n",
    "\n",
    "By Ergodic Theorem\n",
    "$$Q_0(\\theta)=-\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]'\\hat{\\bf{W}}\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]$$\n",
    "\n",
    "#### (Q2) What are the conditions for an M-estimator $\\hat{\\theta}$ to be well-defined?\n",
    "1. $g(\\bf{w}_i;\\theta)$ is a continuous function of $\\theta$ for any data $\\bf{w}$\n",
    "2. $g(\\bf{w}_i;\\theta)$ is a measurable function of $\\bf{w}$ for all $\\theta \\in \\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of GMM Estimator \n",
    "#### (Q3)  What is the identification condition for an GMM estimator?\n",
    "\n",
    "- Notice that the maximum is zero at $\\theta_0$, because of the orthogonality conditions, $\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big]=0$.\n",
    "- Therefore, the identification is satisfied if for all $\\theta \\in \\Theta$, \n",
    "$$\\mathbb{E}\\big[g(\\bf{w}_i;\\theta)\\big] \\neq \\mathbb{E}\\big[g(\\bf{w}_i;\\theta_0)\\big]$$\n",
    "\n",
    "#### (Q4) What is the uniform convergence condition?\n",
    "$$\\mathbb{E}\\left[\\sup_{\\theta \\in \\Theta} \\vert\\vert g(\\bf{w}_i;\\theta) \\vert\\vert \\right] < \\infty$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
