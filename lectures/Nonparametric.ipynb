{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparametric estimation is also referred as curve estimation or smoothing. We do not make any functional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff\n",
    "\n",
    "Let $g$ denote an unknown function such as a density function or a regression\n",
    "function. Let $\\hat{g_n}$ denote an estimator of $g$. Bear in mind that $\\hat{g_n}$ is a random\n",
    "function evaluated at a point x. The estimator is random because it depends\n",
    "on the data.\n",
    "\n",
    "![alt text](../images/nonparametricfun.png)\n",
    "\n",
    "\n",
    "A loss function, we will use integrated squared error (ISE):\n",
    "$$L(g,\\hat{g_n}) = \\int (g(u)-\\hat{g_n}(u))^2$$\n",
    "\n",
    "The **risk** or mean integrated squared error (MISE) with respect to squared error loss is\n",
    "$$R(f,\\hat{f}) = E(L(g,\\hat{g}))$$\n",
    "\n",
    "Then, the risk can be written as\n",
    "$$R(f,\\hat{f}) = \\int b^2(x)dx + \\int v(x)dx$$\n",
    "where \n",
    "$$b(x) = E(\\hat{g_n})-g(x)$$\n",
    "is the bias of $\\hat{g_n}(x)$ at a fixed $x$ and \n",
    "$$v(x) = Var(\\hat{g_n}(x)) = E( [\\hat{g_n} - E(\\hat{g_n})]^2 )$$\n",
    "is the variance of $\\hat{g_n}(x)$ at a fixed $x$.\n",
    "\n",
    "In summary, \n",
    "\n",
    "$$\\text{RISK} = \\text{BIAS}^2 + \\text{VARIANCE}$$\n",
    "\n",
    "When the data are oversmoothed, the bias term is large and the variance is small. When the data are undersmoothed the opposite is true. \n",
    "\n",
    "This is called the bias-variance tradeoff.\n",
    "\n",
    "![alt text](../images/biasvariance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Regression Estimators\n",
    "\n",
    "\n",
    "### Nadaraya-Watson Kernel Regression\n",
    "\n",
    "\n",
    "Kernel regression estimation is an example of fully nonparametric\n",
    "estimation (others are splines, nearest neighbors, etc.). We'll consider\n",
    "the Nadaraya-Watson kernel regression estimator in a simple case.\n",
    "\n",
    "Suppose we have an iid sample from the joint density $f(x,y),$ where\n",
    "$x$ is $k$ -dimensional. The model is \n",
    "$$\n",
    "y_{t}=g(x_{t})+\\varepsilon_{t},\n",
    "$$\n",
    " where \n",
    "$$\n",
    "E(\\varepsilon_{t}|x_{t})=0.\n",
    "$$\n",
    "\n",
    "The conditional expectation of $y$ given $x$ is $g(x).$ By definition\n",
    "of the conditional expectation, we have \n",
    "\\begin{eqnarray*}\n",
    "g(x) & = & \\int y\\frac{f(x,y)}{h(x)}dy\\\\\n",
    " & = & \\frac{1}{h(x)}\\int yf(x,y)dy,\n",
    "\\end{eqnarray*}\n",
    " where $h(x)$ is the marginal density of $x:$\n",
    "$$\n",
    "h(x)=\\int f(x,y)dy.\n",
    "$$\n",
    "\n",
    "\n",
    "This suggests that we could estimate $g(x)$ by estimating $h(x)$\n",
    "and $\\int yf(x,y)dy.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of the denominator\n",
    "\n",
    "A kernel estimator for $h(x)$ has the form \n",
    "$$\n",
    "\\hat{h}(x)=\\frac{1}{n}\\sum_{t=1}^{n}\\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k}},\n",
    "$$\n",
    " where $n$ is the sample size and $k$ is the dimension of $x.$\n",
    "\n",
    "\n",
    "The function $K(\\cdot)$ (the kernel) is absolutely integrable: \n",
    "$$\n",
    "\\int|K(x)|dx<\\infty,\n",
    "$$\n",
    " and $K(\\cdot)$ integrates to $1:$\n",
    "$$\n",
    "\\int K(x)dx=1.\n",
    "$$\n",
    "In this respect, $K(\\cdot)$ is like a density function, but we do\n",
    "not necessarily restrict $K(\\cdot)$ to be nonnegative.\n",
    "\n",
    "\n",
    "The window width parameter, $\\gamma_{n}$ is a sequence of\n",
    "positive numbers that satisfies \n",
    "\\begin{eqnarray*}\n",
    "\\lim_{n\\rightarrow\\infty}\\gamma_{n} & = & 0\\\\\n",
    "\\lim_{n\\rightarrow\\infty}n\\gamma_{n}^{k} & = & \\infty\n",
    "\\end{eqnarray*}\n",
    " So, the window width must tend to zero, but not too quickly.\n",
    " \n",
    " \n",
    "To show **pointwise consistency** of $\\hat{h}(x)$ for $h(x),$ first\n",
    "consider the expectation of the estimator (because the estimator is\n",
    "an average of iid terms, we only need to consider the expectation\n",
    "of a representative term): \n",
    "$$\n",
    "E\\left[\\hat{h}(x)\\right]=\\int\\gamma_{n}^{-k}K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]h(z)dz.\n",
    "$$\n",
    " Change variables as $z^{*}=(x-z)/\\gamma_{n},$ so $z=x-\\gamma_{n}z^{*}$\n",
    "and $|\\frac{dz}{dz^{*\\prime}}|=\\gamma_{n}^{k},$ we obtain\n",
    "\\begin{eqnarray*}\n",
    "E\\left[\\hat{h}(x)\\right] & = & \\int\\gamma_{n}^{-k}K\\left(z^{*}\\right)h(x-\\gamma_{n}z^{*})\\gamma_{n}^{k}dz^{*}\\\\\n",
    " & = & \\int K\\left(z^{*}\\right)h(x-\\gamma_{n}z^{*})dz^{*}.\n",
    "\\end{eqnarray*}\n",
    " Now, asymptotically, \n",
    "\\begin{eqnarray*}\n",
    "\\lim_{n\\rightarrow\\infty}E\\left[\\hat{h}(x)\\right] & = & \\lim_{n\\rightarrow\\infty}\\int K\\left(z^{*}\\right)h(x-\\gamma_{n}z^{*})dz^{*}\\\\\n",
    " & = & \\int\\lim_{n\\rightarrow\\infty}K\\left(z^{*}\\right)h(x-\\gamma_{n}z^{*})dz^{*}\\\\\n",
    " & = & \\int K\\left(z^{*}\\right)h(x)dz^{*}\\\\\n",
    " & = & h(x)\\int K\\left(z^{*}\\right)dz^{*}\\\\\n",
    " & = & h(x),\n",
    "\\end{eqnarray*}\n",
    " since $\\gamma_{n}\\rightarrow0$ and $\\int K\\left(z^{*}\\right)dz^{*}=1$\n",
    "by assumption. (Note:\\ that we can pass the limit through the integral\n",
    "is a result of the dominated convergence theorem. For this to hold\n",
    "we need that $h(\\cdot)$ be dominated by an absolutely integrable\n",
    "function.)\n",
    "\n",
    "\n",
    "Next, considering the **variance** of $\\hat{h}(x),$ we have, due to the\n",
    "iid assumption \n",
    "\\begin{eqnarray*}\n",
    "n\\gamma_{n}^{k}V\\left[\\hat{h}(x)\\right] & = & n\\gamma_{n}^{k}\\frac{1}{n^{2}}\\sum_{t=1}^{n}V\\left\\{ \\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k}}\\right\\} \\\\\n",
    " & = & \\gamma_{n}^{-k}\\frac{1}{n}\\sum_{t=1}^{n}V\\left\\{ K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]\\right\\} \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "By the representative term argument, this is\n",
    "$$\n",
    "n\\gamma_{n}^{k}V\\left[\\hat{h}(x)\\right]=\\gamma_{n}^{-k}V\\left\\{ K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]\\right\\} \n",
    "$$\n",
    "\n",
    "\\item Also, since $V(x)=E(x^{2})-E(x)^{2}$ we have \n",
    "\\begin{eqnarray*}\n",
    "n\\gamma_{n}^{k}V\\left[\\hat{h}(x)\\right] & = & \\gamma_{n}^{-k}E\\left\\{ \\left(K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]\\right)^{2}\\right\\} -\\gamma_{n}^{-k}\\left\\{ E\\left(K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]\\right)\\right\\} ^{2}\\\\\n",
    " & = & \\int\\gamma_{n}^{-k}K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]^{2}h(z)dz-\\gamma_{n}^{k}\\left\\{ \\int\\gamma_{n}^{-k}K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]h(z)dz\\right\\} ^{2}\\\\\n",
    " & = & \\int\\gamma_{n}^{-k}K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]^{2}h(z)dz-\\gamma_{n}^{k}E\\left[\\widehat{h}(x)\\right]^{2}\n",
    "\\end{eqnarray*}\n",
    " The second term converges to zero: \n",
    "$$\n",
    "\\gamma_{n}^{k}E\\left[\\widehat{h}(x)\\right]^{2}\\rightarrow0,\n",
    "$$\n",
    " by the previous result regarding the expectation and the fact that\n",
    "$\\gamma_{n}\\rightarrow0.$ Therefore, \n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}n\\gamma_{n}^{k}V\\left[\\hat{h}(x)\\right]=\\lim_{n\\rightarrow\\infty}\\int\\gamma_{n}^{-k}K\\left[\\left(x-z\\right)/\\gamma_{n}\\right]^{2}h(z)dz.\n",
    "$$\n",
    " Using exactly the same change of variables as before, this can be\n",
    "shown to be \n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}n\\gamma_{n}^{k}V\\left[\\hat{h}(x)\\right]=h(x)\\int\\left[K(z^{*})\\right]^{2}dz^{*}.\n",
    "$$\n",
    " Since both $\\int\\left[K(z^{*})\\right]^{2}dz^{*}$ and $h(x)$ are\n",
    "bounded, the RHS is bounded, and since $n\\gamma_{n}^{k}\\rightarrow\\infty$\n",
    "by assumption, we have that \n",
    "$$\n",
    "V\\left[\\hat{h}(x)\\right]\\rightarrow0.\n",
    "$$\n",
    "\n",
    "Since the bias and the variance both go to zero, we have **pointwise\n",
    "consistency** (convergence in quadratic mean implies convergence in\n",
    "probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of the numerator\n",
    "\n",
    "To estimate $\\int yf(x,y)dy,$ we need an estimator of $f(x,y).$\n",
    "The estimator has the same form as the estimator for $h(x),$ only\n",
    "with one dimension more: \n",
    "$$\n",
    "\\hat{f}(x,y)=\\frac{1}{n}\\sum_{t=1}^{n}\\frac{K_{*}\\left[\\left(y-y_{t}\\right)/\\gamma_{n},\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k+1}}\n",
    "$$\n",
    " The kernel $K_{*}\\left(\\cdot\\right)$ is required to have mean zero:\n",
    "$$\n",
    "\\int yK_{*}\\left(y,x\\right)dy=0\n",
    "$$\n",
    " and to marginalize to the previous kernel for $h(x):$\n",
    "$$\n",
    "\\int K_{*}\\left(y,x\\right)dy=K(x).\n",
    "$$\n",
    " With this kernel, we have (not obviously, see Li and Racine, Ch.\n",
    "2, section 2.1) \n",
    "$$\n",
    "\\int y\\hat{f}(y,x)dy=\\frac{1}{n}\\sum_{t=1}^{n}y_{t}\\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k}}\n",
    "$$\n",
    " by marginalization of the kernel, so we obtain \n",
    "\\begin{eqnarray*}\n",
    "\\hat{g}(x) & := & \\frac{1}{\\hat{h}(x)}\\int y\\hat{f}(y,x)dy\\\\\n",
    " & = & \\frac{\\frac{1}{n}\\sum_{t=1}^{n}y_{t}\\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k}}}{\\frac{1}{n}\\sum_{t=1}^{n}\\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\gamma_{n}^{k}}}\\\\\n",
    " & = & \\frac{\\sum_{t=1}^{n}y_{t}K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\sum_{t=1}^{n}K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}\n",
    "\\end{eqnarray*}\n",
    " This is the Nadaraya-Watson kernel regression estimator.\n",
    "\n",
    "\n",
    "#### Kernel Regression\n",
    "\n",
    "Defining:\n",
    "$$\n",
    "w_{t}=\\frac{K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]}{\\sum_{t=1}^{n}K\\left[\\left(x-x_{t}\\right)/\\gamma_{n}\\right]},\n",
    "$$\n",
    "the kernel regression estimator for $g(x_{t})$ can be written as\n",
    "\\begin{align*}\n",
    "\\hat{g}(x) & =\\sum_{t=1}^{n}y_{t}w_{t},\n",
    "\\end{align*}\n",
    "a weighted average of the $y_{j},\\,j=1,2,...,n$, where higher weights\n",
    "are associated with points that are closer to $x_{t}.$ \n",
    "\n",
    "- The window width parameter $\\gamma_{n}$ imposes smoothness. The estimator\n",
    "is increasingly flat as $\\gamma_{n}\\rightarrow\\infty,$ since in this\n",
    "case each weight tends to $1/n.$\n",
    "- A large window width reduces the variance (strong imposition of flatness),\n",
    "but increases the bias.\n",
    "- A small window width reduces the bias, but makes very little use of\n",
    "information except points that are in a small neighborhood of $x_{t}.$\n",
    "Since relatively little information is used, the variance is large\n",
    "when the window width is small.\n",
    "- The standard normal density is a popular choice for $K(.)\\;$ and\n",
    "$K_{*}(y,x),$ though there are possibly better alternatives. \n",
    "\n",
    "\n",
    "One can choose the window width using Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Nadaraya-Watson Estimator with Guassian Kernel and Silverman's Rule of Thumb Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "k=2\n",
    "ydata = rand(Bernoulli(0.5),n)\n",
    "xdata = randn(n,k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nw_ap (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nw_ap(xeval,xdata,ydata)\n",
    "    h = 1.06.*std(xdata,dims=1).*(size(xdata,1)^-0.2) #Silverman's rule of thumb\n",
    "    input = (repeat(randn(2)',size(xdata,1)) .- xdata) ./ repeat(h,size(xdata,1))\n",
    "    phi = exp.(-0.5.*(input).^2)./sqrt(2*pi)\n",
    "    phiprod = prod(phi,dims=2).*(1/(prod(h)*size(xdata,1)))\n",
    "\n",
    "    return sum(phiprod.*ydata) / sum(phiprod)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47560938381448953"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_ap(randn(2),xdata,ydata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
