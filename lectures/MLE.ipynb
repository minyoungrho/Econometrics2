{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Optim\n",
    "using GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "The maximum likelihood estimator is important because it uses all of the information in a fully\n",
    "specified statistical model.\n",
    " - Asymptotically efficient\n",
    " - Possible misspecification of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sample of size $n$ of the random vectors $y$ and $z$. Suppose the joint density of $Y=\\big( y_1,\\dots,y_n \\big)$ and $Z=\\big( z_1,\\dots,z_n \\big)$ is characterized by a parameter vector $psi_0$:\n",
    "$$f_YZ(Y,Z,\\psi_0)$$\n",
    "The **likelihood function** is just this density evaluated at other values of $\\psi$:\n",
    "$$L(Y,Z,\\psi)=f(Y,Z,\\psi), \\quad \\psi \\in \\Psi$$\n",
    "where $\\Psi$ is a parameter space.\n",
    "The **maximum likelihood estimator**, $\\psi_0$, is the value of $psi$ that maximizes the likelihood function:\n",
    "$$\\psi_0 = \\arg \\max_{\\psi \\in \\Psi} L(Y,Z,\\psi)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (Count data)\n",
    "Suppose we have a sample $Y={y_1,\\dots,y_n}$ where the data are counts: the number of times some event occurs in a given interval of time, e.g., number of visits to the doctor a year. The simplest count data density is the Poisson:\n",
    "$$f_Y(y;\\lambda)=\\frac{e^{-\\lambda}\\lambda^y}{y!}$$\n",
    "If the observations are i.i.d. according to the above density, then the joint density of the sample is:\n",
    "$$L = \\prod_{i=1}^{n}\\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!}$$\n",
    "A little calculus and algebra shows that:\n",
    "$$\\lambda_0 = \\bar{y} = \\arg\\max_{\\lambda \\in \\Lambda} L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exogenous variables\n",
    "The likelihood function can be factored as \n",
    "$$f_{YZ}(Y,Z,\\psi) = f_{Y\\vert Z}(Y \\vert Z,\\theta)f_Z(Z,\\rho)$$\n",
    "where $\\theta$ are whatever elements of $\\psi$ that happen to enter in the conditional density, and $\\rho$ are the elements that enter into the marginal density.\n",
    "\n",
    "If $\\theta$ and $\\rho$ share no elements, then the maximizer of the conditional likelihood function $f_{Y \\vert Z}(Y \\vert Z,\\theta)$ with respect to $\\theta$ is the same as the maximizer of the overall likelihood function $f_{YZ}(Y,Z,\\psi) = f_{Y \\vert Z}(Y \\vert Z,\\theta)f_Z(Z,\\rho)$, for the elements of $\\psi$ that corresponds to $\\theta$.\n",
    "\n",
    "$Z$ are said to be exogenous for estimation of $\\theta$, and may more conveniently work with the conditional likelihood function $f_{Y \\vert Z}(Y \\vert Z,\\theta)$ for the purpose of estimating $\\theta$.\n",
    "\n",
    "### Factorization of the Likelihood Function\n",
    "- If the $n$ observations are independent, the likelihood function can be written as \n",
    "$$L(Y \\vert Z,\\theta) = \\prod_{i=1}^{n} f(y_i \\vert z_i,\\theta)$$\n",
    "\n",
    "- If not, we can factor the likelihood into:\n",
    "$$\n",
    "L(Y|Z,\\theta)=f(y_{n}|y_{1,}y_{2},\\ldots y_{n-1},Z,\\theta)f(y_{n-1}|y_{1,}y_{2},\\ldots y_{n-2},Z,\\theta)\\cdots f(y_{2}|y_{1},Z,\\theta)f(y_{1}|Z,\\theta)\n",
    "$$ using\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\underbrace{f(y_{1,}y_{2},\\ldots y_{n-1},y_{n}|Z,\\theta)}\\\\\n",
    "\\mathrm{joint}\n",
    "\\end{array}=\\begin{array}{c}\n",
    "\\underbrace{f(y_{n}|y_{1,}y_{2},\\ldots y_{n-1},Z,\\theta)}\\\\\n",
    "\\mathrm{conditional}\n",
    "\\end{array}\\begin{array}{c}\n",
    "\\underbrace{f(y_{1,}y_{2},\\ldots y_{n-1}|Z,\\theta)}\\\\\n",
    "\\mathrm{marginal}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (Bernoulli trial)\n",
    "\n",
    "Suppose that we are flipping a coin that may be biased, so that the probability of a heads may not be 0.5. Maybe we are interested in estimating the probability of heads. Let $Y$ be a binary variable that equals 1 in case of heads and 0 in case of tails. The outcome of a coin toss ia a Bernoulli random variable:\n",
    "$$f_{Y}(y,p)=p^{y}\\left(1-p\\right)^{1-y}$$\n",
    "The average log-likelihood function is:\n",
    "$$Q_{n}(p)=\\frac{1}{n}\\sum_{t=1}^{n}y_{t}\\ln p+\\left(1-y_{t}\\right)\\ln\\left(1-p\\right)$$\n",
    "The derivative of a representative term is:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial\\ln f_{Y}(y,p)}{\\partial p} & = & \\frac{y}{p}-\\frac{\\left(1-y\\right)}{\\left(1-p\\right)}\\\\\n",
    " & = & \\frac{y-p}{p\\left(1-p\\right)}\n",
    "\\end{eqnarray*}\n",
    "Hence,\n",
    "$$\n",
    "\\frac{\\partial Q_{n}(p)}{\\partial p}=\\frac{1}{n}\\sum_{t=1}^{n}\\frac{y_{t}-p}{p\\left(1-p\\right)}.\n",
    "$$\n",
    "Setting to zero and solving:\n",
    "$$\\hat{p}=\\bar{y}$$\n",
    "It's easy to calculate $p$ using MLE in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that it is consistent:\n",
    "- For a given $p$, the objective function converges to the limit of its expectation\n",
    " $$Q_{n}(p)=\\frac{1}{n}\\sum_{t=1}^{n}y_{t}\\ln p+\\left(1-y_{t}\\right)\\ln\\left(1-p\\right) \n",
    " \\rightarrow p_0 \\ln p + (1-p_0)\\ln(1-p)$$\n",
    "- Compact parameter space: $p_0$ lies between 0 and 1\n",
    "- Objective function continuous and measurable:  as long as $p \\in (0,1)$ the objective function is bounded\n",
    "\n",
    "It is easy to see that $p$ that maximizes $$\\frac{\\partial Q_n}{\\partial p} = \\frac{p_{0}}{p}+\\frac{(1-p_{0}}{\\ln(1-p)}$$ is $p_0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (classical linear regression model)\n",
    "\n",
    "Let's suppose that a dependent variable is normally distributed: $y \\sim N(\\mu_0,\\sigma_0^2)$, so\n",
    "$$\n",
    "f_{y}(y;\\mu_{0},\\sigma_{0}^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}}\\exp\\left(-\\frac{(y-\\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)\n",
    "$$\n",
    "Suppose that the mean, $\\mu_{0}$, depends on some regressors, $x$.\n",
    "The simplest way to do this is to assume that $\\mu_{0}=x^{\\prime}\\beta_{0}.$\n",
    "With this, the density, conditional on $x$ is \n",
    "$$\n",
    "f_{y}(y|x;\\beta_{0},\\sigma_{0}^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}}\\exp\\left(-\\frac{(y-x^{\\prime}\\beta_{0})^{2}}{2\\sigma_{0}^{2}}\\right)\n",
    "$$\n",
    "This is an example of **parameterization** of a density, making\n",
    "some parameters depend on additional variables and new parameters.\n",
    "With an i.i.d. sample of size $n$, the overall conditional density\n",
    "is the product of the conditional density of each observation: \n",
    "$$\n",
    "f_{y}(y_{1},y_{2},...,y_{n}|x_{1},x_{2},...,x;\\beta_{0},\\sigma_{0}^{2})=\\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}}\\exp\\left(-\\frac{(y_{t}-x_{t}^{\\prime}\\beta_{0})^{2}}{2\\sigma_{0}^{2}}\\right)\n",
    "$$\n",
    "Taking logarithms, and evaluating at some point in the parameter\n",
    "space, we get the log-likelihood function: \n",
    "$$\n",
    "\\ln L(Y|X;\\beta,\\sigma^{2})=-n\\ln\\sqrt{2\\pi}-n\\ln\\sigma-\\sum_{t=1}^{n}\\frac{\\left(y_{t}-x_{t}'\\beta\\right)^{2}}{2\\sigma^{2}}\n",
    "$$\n",
    "\n",
    "- Observe that the first order conditions for $\\beta$ are the same for the OLS estimator. \n",
    "- We know that the OLS estimator is consistent without making distributional assumptions regarding the errors.  Thus, estimator for MLE will also be consistent.\n",
    "\n",
    "Let's verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true parameters: [1.0, -1.0, 0.0, 1.0]\n",
      "the ML estimates: [0.6527227313470814, -2.087986882691496, 0.8335550844464494, 0.85279748155378]\n",
      "the OLS estimates: [0.6527227308279165; -2.0879868827116117; 0.8335550858127168]\n"
     ]
    }
   ],
   "source": [
    "using Optim\n",
    "# example showing how to do maximum likelihood estimation\n",
    "# for data generated by CLRM with normality\n",
    "# sample size\n",
    "using Econometrics, Statistics\n",
    "n = 10\n",
    "\n",
    "# random true parameters\n",
    "theta = [1.0,-1.0,0.0,1.0]\n",
    "b = theta[1:3]\n",
    "sig = theta[4]\n",
    "\n",
    "# generate random data\n",
    "x = [ones(n,1) rand(n,2)]\n",
    "e = sig*randn(n,1)\n",
    "y = x*b + e\n",
    "\n",
    "function normal(theta, y, x)\n",
    "    b = theta[1:end-1]\n",
    "    s = theta[end][1]\n",
    "    e = (y - x*b)./s\n",
    "    logdensity = -log.(sqrt.(2.0*pi)) .- 0.5*log(s.^2) .- 0.5*e.*e\n",
    "end\n",
    "function fminunc(obj, x; tol = 1e-08)\n",
    "    results = Optim.optimize(obj, x, LBFGS(), \n",
    "                            Optim.Options(\n",
    "                            g_tol = tol,\n",
    "                            x_tol=tol,\n",
    "                            f_tol=tol))\n",
    "    return results.minimizer, results.minimum, Optim.converged(results)\n",
    "    #xopt, objvalue, flag = fmincon(obj, x, tol=tol)\n",
    "    #return xopt, objvalue, flag\n",
    "end\n",
    "\n",
    "# do ML: note minus sign, also, do \"edit(normal)\" to see what's done\n",
    "obj = theta -> -mean(normal(theta, y, x))\n",
    "thetahat, junk, junk = fminunc(obj, theta)\n",
    "\n",
    "# results\n",
    "println(\"the true parameters: \", theta)\n",
    "println(\"the ML estimates: \", thetahat)\n",
    "println(\"the OLS estimates: \", inv(x'*x)*x'*y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency: general result\n",
    "\n",
    "$$\n",
    "\\mathcal{E}\\left(\\ln\\left(\\frac{L(\\theta)}{L(\\theta_{0})}\\right)\\right)\\leq\\ln\\left(\\mathcal{E}\\left(\\frac{L(\\theta)}{L(\\theta_{0})}\\right)\\right)\n",
    "$$\n",
    " by [Jensen's inequality](http://en.wikipedia.org/wiki/Jensen's_inequality)\n",
    "($\\ln\\left(\\cdot\\right)$ is a concave function).\n",
    "\n",
    "Now, the expectation on the RHS is \n",
    "$$\n",
    "\\mathcal{E}\\left(\\frac{L(\\theta)}{L(\\theta_{0})}\\right)=\\int\\frac{L(\\theta)}{L(\\theta_{0})}L(\\theta_{0})dy=1,\n",
    "$$\n",
    " since $L(\\theta_{0})$ \\emph{is} the density function of the observations,\n",
    "and since the integral of any density is 1$.$ \\newpage Therefore,\n",
    "since $\\ln(1)=0,$\n",
    "$$\n",
    "\\mathcal{E}\\left(\\ln\\left(\\frac{L(\\theta)}{L(\\theta_{0})}\\right)\\right)\\leq0,\n",
    "$$\n",
    " or\n",
    " $$\n",
    "\\begin{align*}\n",
    "\\mathcal{E}\\left(s_{n}\\left(\\theta\\right)\\right)-\\mathcal{E}\\left(s_{n}\\left(\\theta_{0}\\right)\\right) & \\leq0\n",
    "\\end{align*}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\mathcal{E}\\left(s_{n}\\left(\\theta\\right)\\right)\\leq\\mathcal{E}\\left(s_{n}\\left(\\theta_{0}\\right)\\right)\n",
    "$$\n",
    "Taking limits of each side:\n",
    "$$\n",
    "s_{\\infty}(\\theta)\\leq s_{\\infty}(\\theta_{0})\n",
    "$$\n",
    "except on a set of zero probability.\n",
    "\n",
    "So the true parameter value is the maximizer of the limiting objective\n",
    "function (we are in Case 1 of the three cases discussed above - a\n",
    "fully correctly specified model).\n",
    "\n",
    "If the identification assumption holds, then there is a unique maximizer,\n",
    "so the inequality is strict if $\\theta\\neq\\theta_{0}$: \n",
    "$$\n",
    "s_{\\infty}(\\theta)<s_{\\infty}(\\theta_{0}),\\forall\\theta\\neq\\theta_{0},\\text{a.s.}\n",
    "$$\n",
    "Therefore, $\\theta_{0}$ is the unique maximizer of $s_{\\infty}(\\theta),$\n",
    "and thus, \n",
    "$$\n",
    "\\lim_{n\\rightarrow\\infty}\\hat{\\theta}=\\theta_{0},\\textrm{ a}.\\textrm{s}.\n",
    "$$\n",
    " So, the ML estimator is consistent for the true parameter value.\n",
    "In practice, we will need to check identification for the specific\n",
    "model under consideration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that by increasing the n in the above example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic Normality: general result\n",
    "\n",
    "You will go over with with TA on Friday :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
