{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using GLM\n",
    "using Optim\n",
    "using Statistics\n",
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nerlove Model\n",
    "\n",
    "### Theoretical Background\n",
    "For a firm that takes input prices $w$ and the output level $q$\n",
    "as given, the cost minimization problem is to choose the quantities\n",
    "of inputs $x$ to solve the problem\n",
    "$$\n",
    "\\min_{x}w'x\n",
    "$$\n",
    " subject to the restriction\n",
    "$$\n",
    "f(x)=q.\n",
    "$$\n",
    " The solution is the vector of factor demands $x(w,q)$. The cost\n",
    "function is obtained by substituting the factor demands into the\n",
    "criterion function: \n",
    "$$\n",
    "Cw,q)=w'x(w,q).\n",
    "$$\n",
    " \n",
    "- **Monotonicity** Increasing factor prices cannot decrease cost, so \n",
    "$$\\frac{\\partial C(w,q)}{\\partial w}\\geq0$$\n",
    "Remember that these derivatives give the conditional factor demands\n",
    "(Shephard's Lemma).\n",
    "- **Homogeneity** The cost function is homogeneous of degree 1 in input prices: $C(tw,q)=tC(w,q)$ where $t$ is a scalar constant. This is because the factor demands are homogeneous of degree zero in factor prices - they only depend upon relative prices.\n",
    "- **Returns to scale** The returns to scale parameter $\\gamma$ is defined as the inverse of the elasticity of cost with respect to output:\n",
    "$$\n",
    "\\gamma=\\left(\\frac{\\partial C(w,q)}{\\partial q}\\frac{q}{C(w,q)}\\right)^{-1}\n",
    "$$\n",
    "Constant returns to scale is the case where increasing production\n",
    "$q$ implies that cost increases in the proportion 1:1. If this is\n",
    "the case, then $\\gamma=1$.\n",
    "\n",
    "#### Cobb-Douglas functional form\n",
    "\n",
    "The Cobb-Douglas functional form is linear in the logarithms of the\n",
    "regressors and the dependent variable. For a cost function, if there\n",
    "are $g$ factors, the Cobb-Douglas cost function has the form\n",
    "\n",
    "$$\n",
    "C=Aw_{1}^{\\beta_{1}}...w_{g}^{\\beta_{g}}q^{\\beta_{q}}e^{\\varepsilon}\n",
    "$$\n",
    "What is the elasticity of $C$ with respect to $w_{j}$?\n",
    "\\begin{eqnarray*}\n",
    "e_{w_{j}}^{C} & = & \\left(\\frac{\\partial C}{\\partial_{W_{J}}}\\right)\\left(\\frac{w_{j}}{C}\\right)\\\\\n",
    " & = & \\beta_{j}Aw_{1}^{\\beta_{1}}.w_{j}^{\\beta_{j}-1}..w_{g}^{\\beta_{g}}q^{\\beta_{q}}e^{\\varepsilon}\\frac{w_{j}}{Aw_{1}^{\\beta_{1}}...w_{g}^{\\beta_{g}}q^{\\beta_{q}}e^{\\varepsilon}}\\\\\n",
    " & = & \\beta_{j}\n",
    "\\end{eqnarray*}\n",
    "This is one of the reasons the Cobb-Douglas form is popular - the\n",
    "coefficients are easy to interpret, since they are the elasticities\n",
    "of the dependent variable with respect to the explanatory variable.\n",
    "Not that in this case,\n",
    "\\begin{eqnarray*}\n",
    "e_{w_{j}}^{C} & = & \\left(\\frac{\\partial C}{\\partial_{W_{J}}}\\right)\\left(\\frac{w_{j}}{C}\\right)\\\\\n",
    " & = & x_{j}(w,q)\\frac{w_{j}}{C}\\\\\n",
    " & \\equiv & s_{j}(w,q)\n",
    "\\end{eqnarray*}\n",
    "the cost share of the $j^{th}$ input. So with a Cobb-Douglas\n",
    "cost function, $\\beta_{j}=s_{j}(w,q)$. The cost shares are constants.\n",
    "\n",
    "Note that after a logarithmic transformation we obtain\n",
    "$$\n",
    "\\ln C=\\alpha+\\beta_{1}\\ln w_{1}+...+\\beta_{g}\\ln w_{g}+\\beta_{q}\\ln q+\\epsilon\n",
    "$$\n",
    "where $\\alpha=\\ln A$ . So we see that the transformed model is linear\n",
    "in the logs of the data.\n",
    "\n",
    "One can verify that the property of HOD1 implies that \n",
    "$$\n",
    "\\sum_{i=1}^{g}\\beta_{i}=1\n",
    "$$\n",
    "In other words, the cost shares add up to 1. \n",
    "\n",
    "The hypothesis that the technology exhibits CRTS implies that \n",
    "$$\n",
    "\\gamma=\\frac{1}{\\beta_{q}}=1\n",
    "$$\n",
    "so $\\beta_{q}=1.$ Likewise, monotonicity implies that the coefficients\n",
    "$\\beta_{i}\\geq0,i=1,...,g$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nerlove Data\n",
    "The file contains data on 145 electric utility companies' cost of production,\n",
    "output and input prices. The data are for the U.S., and were collected\n",
    "by M. Nerlove. The observations are by row, and the columns are \n",
    "- COMPANYCOST $(C)$\n",
    "- OUTPUT $(Q)$ \n",
    "- PRICE OF LABOR $(P_{L})$\n",
    "- PRICE OF FUEL $(P_{F})$\n",
    "- PRICE OF CAPITAL$(P_{K})$ \n",
    "\n",
    "Note that the data are sorted by output level (the third column).\n",
    "\n",
    "We will estimate the Cobb-Douglas model \n",
    "$$\\ln C=\\beta_{1}+\\beta_{Q}\\ln Q+\\beta_{L}\\ln P_{L}+\\beta_{F}\\ln P_{F}+\\beta_{K}\\ln P_{K}+\\epsilon\\label{simple nerlove model}\n",
    "$$ by OLS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>firm</th><th>cost</th><th>output</th><th>labor</th><th>fuel</th><th>capital</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Int64</th></tr></thead><tbody><p>6 rows × 6 columns</p><tr><th>1</th><td>101</td><td>0.082</td><td>2</td><td>2.09</td><td>17.9</td><td>183</td></tr><tr><th>2</th><td>102</td><td>0.661</td><td>3</td><td>2.05</td><td>35.1</td><td>174</td></tr><tr><th>3</th><td>103</td><td>0.99</td><td>4</td><td>2.05</td><td>35.1</td><td>171</td></tr><tr><th>4</th><td>104</td><td>0.315</td><td>4</td><td>1.83</td><td>32.2</td><td>166</td></tr><tr><th>5</th><td>105</td><td>0.197</td><td>5</td><td>2.12</td><td>28.6</td><td>233</td></tr><tr><th>6</th><td>106</td><td>0.098</td><td>9</td><td>2.12</td><td>28.6</td><td>195</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& firm & cost & output & labor & fuel & capital\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 101 & 0.082 & 2 & 2.09 & 17.9 & 183 \\\\\n",
       "\t2 & 102 & 0.661 & 3 & 2.05 & 35.1 & 174 \\\\\n",
       "\t3 & 103 & 0.99 & 4 & 2.05 & 35.1 & 171 \\\\\n",
       "\t4 & 104 & 0.315 & 4 & 1.83 & 32.2 & 166 \\\\\n",
       "\t5 & 105 & 0.197 & 5 & 2.12 & 28.6 & 233 \\\\\n",
       "\t6 & 106 & 0.098 & 9 & 2.12 & 28.6 & 195 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m firm  \u001b[0m\u001b[1m cost    \u001b[0m\u001b[1m output \u001b[0m\u001b[1m labor   \u001b[0m\u001b[1m fuel    \u001b[0m\u001b[1m capital \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64  \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64   \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────\n",
       "   1 │   101    0.082       2     2.09     17.9      183\n",
       "   2 │   102    0.661       3     2.05     35.1      174\n",
       "   3 │   103    0.99        4     2.05     35.1      171\n",
       "   4 │   104    0.315       4     1.83     32.2      166\n",
       "   5 │   105    0.197       5     2.12     28.6      233\n",
       "   6 │   106    0.098       9     2.12     28.6      195"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DataFrame(CSV.File(\"../data/nerlove.csv\"))\n",
    "first(data,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>cost</th><th>output</th><th>labor</th><th>fuel</th><th>capital</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>6 rows × 5 columns</p><tr><th>1</th><td>-2.50104</td><td>0.693147</td><td>0.737164</td><td>2.8848</td><td>5.20949</td></tr><tr><th>2</th><td>-0.414001</td><td>1.09861</td><td>0.71784</td><td>3.5582</td><td>5.15906</td></tr><tr><th>3</th><td>-0.0100503</td><td>1.38629</td><td>0.71784</td><td>3.5582</td><td>5.14166</td></tr><tr><th>4</th><td>-1.15518</td><td>1.38629</td><td>0.604316</td><td>3.47197</td><td>5.11199</td></tr><tr><th>5</th><td>-1.62455</td><td>1.60944</td><td>0.751416</td><td>3.35341</td><td>5.45104</td></tr><tr><th>6</th><td>-2.32279</td><td>2.19722</td><td>0.751416</td><td>3.35341</td><td>5.273</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& cost & output & labor & fuel & capital\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & -2.50104 & 0.693147 & 0.737164 & 2.8848 & 5.20949 \\\\\n",
       "\t2 & -0.414001 & 1.09861 & 0.71784 & 3.5582 & 5.15906 \\\\\n",
       "\t3 & -0.0100503 & 1.38629 & 0.71784 & 3.5582 & 5.14166 \\\\\n",
       "\t4 & -1.15518 & 1.38629 & 0.604316 & 3.47197 & 5.11199 \\\\\n",
       "\t5 & -1.62455 & 1.60944 & 0.751416 & 3.35341 & 5.45104 \\\\\n",
       "\t6 & -2.32279 & 2.19722 & 0.751416 & 3.35341 & 5.273 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m6×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m cost       \u001b[0m\u001b[1m output   \u001b[0m\u001b[1m labor    \u001b[0m\u001b[1m fuel    \u001b[0m\u001b[1m capital \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64    \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────\n",
       "   1 │ -2.50104    0.693147  0.737164  2.8848   5.20949\n",
       "   2 │ -0.414001   1.09861   0.71784   3.5582   5.15906\n",
       "   3 │ -0.0100503  1.38629   0.71784   3.5582   5.14166\n",
       "   4 │ -1.15518    1.38629   0.604316  3.47197  5.11199\n",
       "   5 │ -1.62455    1.60944   0.751416  3.35341  5.45104\n",
       "   6 │ -2.32279    2.19722   0.751416  3.35341  5.273"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = log.(data[:,[:cost,:output,:labor,:fuel,:capital]])\n",
    "first(data,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.CholeskyPivoted{Float64,Array{Float64,2}}}},Array{Float64,2}}\n",
       "\n",
       "cost ~ 1 + output + labor + fuel + capital\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%   Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  -3.5265     1.77437    -1.99    0.0488  -7.03452   -0.0184845\n",
       "output        0.720394   0.0174664  41.24    <1e-79   0.685862   0.754926\n",
       "labor         0.436341   0.291048    1.50    0.1361  -0.139076   1.01176\n",
       "fuel          0.426517   0.100369    4.25    <1e-4    0.228082   0.624952\n",
       "capital      -0.219888   0.339429   -0.65    0.5182  -0.890957   0.45118\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols = lm(@formula(cost~output+labor+fuel+capital),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -3.5265028449802216\n",
       "  0.7203940758797012\n",
       "  0.4363412007892406\n",
       "  0.4265169530627446\n",
       " -0.2198883507567723"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = size(data,1)\n",
    "y = data[:,1]\n",
    "x = data[:,2:end]\n",
    "x[!,:intercept]=ones(size(data,1))\n",
    "x = x[!,[:intercept,:output,:labor,:fuel,:capital]]\n",
    "\n",
    "y = convert(Array,y)\n",
    "x = convert(Array,x)\n",
    "inv(x'*x)*x'*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fminunc (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fminunc(obj, x; tol = 1e-08)\n",
    "    results = Optim.optimize(obj, x, LBFGS(), \n",
    "                            Optim.Options(\n",
    "                            g_tol = tol,\n",
    "                            x_tol=tol,\n",
    "                            f_tol=tol))\n",
    "    return results.minimizer, results.minimum, Optim.converged(results)\n",
    "    #xopt, objvalue, flag = fmincon(obj, x, tol=tol)\n",
    "    #return xopt, objvalue, flag\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normal(theta, y, x)\n",
    "    b = theta[1:end-1]\n",
    "    s = theta[end][1]\n",
    "    e = (y - x*b)./s\n",
    "    logdensity = -log.(sqrt.(2.0*pi)) .- 0.5*log(s.^2) .- 0.5*e.*e\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle(model, θ)\n",
    "    avg_obj = θ -> -mean(vec(model(θ))) # average log likelihood\n",
    "    thetahat, objvalue, converged = fminunc(avg_obj, θ) # do the minimization of -logL\n",
    "    objvalue = -objvalue\n",
    "    obj = θ -> vec(model(θ)) # unaveraged log likelihood\n",
    "    n = size(obj(θ),1) # how many observations?\n",
    "    scorecontrib = ForwardDiff.jacobian(obj, vec(thetahat))\n",
    "    I = cov(scorecontrib)\n",
    "    J = ForwardDiff.hessian(avg_obj, vec(thetahat))\n",
    "    Jinv = inv(J)\n",
    "    V= Jinv*I*Jinv/n\n",
    "    return thetahat, objvalue, V, converged\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: objects of type Array{Float64,1} are not callable\u001b[39m\n\u001b[91mUse square brackets [] for indexing an Array.\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: objects of type Array{Float64,1} are not callable\u001b[39m\n\u001b[91mUse square brackets [] for indexing an Array.\u001b[39m",
      "",
      "Stacktrace:",
      " [1] (::var\"#2#4\"{Array{Float64,1}})(::Array{Float64,1}) at .\\In[8]:2",
      " [2] finite_difference_gradient!(::Array{Float64,1}, ::var\"#2#4\"{Array{Float64,1}}, ::Array{Float64,1}, ::FiniteDiff.GradientCache{Nothing,Nothing,Nothing,Array{Float64,1},Val{:central}(),Float64,Val{true}()}; relstep::Float64, absstep::Float64, dir::Bool) at C:\\Users\\Minyoung Rho\\.julia\\packages\\FiniteDiff\\blirf\\src\\gradients.jl:273",
      " [3] finite_difference_gradient!(::Array{Float64,1}, ::Function, ::Array{Float64,1}, ::FiniteDiff.GradientCache{Nothing,Nothing,Nothing,Array{Float64,1},Val{:central}(),Float64,Val{true}()}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\FiniteDiff\\blirf\\src\\gradients.jl:224",
      " [4] (::NLSolversBase.var\"#g!#15\"{var\"#2#4\"{Array{Float64,1}},FiniteDiff.GradientCache{Nothing,Nothing,Nothing,Array{Float64,1},Val{:central}(),Float64,Val{true}()}})(::Array{Float64,1}, ::Array{Float64,1}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\NLSolversBase\\geyh3\\src\\objective_types\\oncedifferentiable.jl:57",
      " [5] (::NLSolversBase.var\"#fg!#16\"{var\"#2#4\"{Array{Float64,1}}})(::Array{Float64,1}, ::Array{Float64,1}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\NLSolversBase\\geyh3\\src\\objective_types\\oncedifferentiable.jl:61",
      " [6] value_gradient!!(::OnceDifferentiable{Float64,Array{Float64,1},Array{Float64,1}}, ::Array{Float64,1}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\NLSolversBase\\geyh3\\src\\interface.jl:82",
      " [7] initial_state(::LBFGS{Nothing,LineSearches.InitialStatic{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Optim.var\"#17#19\"}, ::Optim.Options{Float64,Nothing}, ::OnceDifferentiable{Float64,Array{Float64,1},Array{Float64,1}}, ::Array{Float64,1}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\Optim\\uwNqi\\src\\multivariate\\solvers\\first_order\\l_bfgs.jl:164",
      " [8] optimize(::OnceDifferentiable{Float64,Array{Float64,1},Array{Float64,1}}, ::Array{Float64,1}, ::LBFGS{Nothing,LineSearches.InitialStatic{Float64},LineSearches.HagerZhang{Float64,Base.RefValue{Bool}},Optim.var\"#17#19\"}, ::Optim.Options{Float64,Nothing}) at C:\\Users\\Minyoung Rho\\.julia\\packages\\Optim\\uwNqi\\src\\multivariate\\optimize\\optimize.jl:35",
      " [9] #optimize#87 at C:\\Users\\Minyoung Rho\\.julia\\packages\\Optim\\uwNqi\\src\\multivariate\\optimize\\interface.jl:142 [inlined]",
      " [10] optimize at C:\\Users\\Minyoung Rho\\.julia\\packages\\Optim\\uwNqi\\src\\multivariate\\optimize\\interface.jl:141 [inlined]",
      " [11] #fminunc#1 at .\\In[6]:2 [inlined]",
      " [12] fminunc at .\\In[6]:2 [inlined]",
      " [13] mle(::Array{Float64,1}, ::Array{Float64,1}) at .\\In[8]:3",
      " [14] top-level scope at In[19]:2",
      " [15] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "theta = [zeros(size(x,2)); 1.0] # start values for estimation\n",
    "thetahat, objvalue, V, converged = mle(normal, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: thetahat not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: thetahat not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      " [2] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "thetahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: converged not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: converged not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      " [2] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "converged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
